# DeepNet for Image Classification using PyTorch
### Spring 2021
*Course project (Permission granted from the instructor)

**Part 1: Improved deep net architecture (BaseNet) to classify (small) images into 100 categories on CIFAR100**

**Part 2: Fine-tuning ResNet model pre-trained on ImageNet for classifying the [Caltech-UCSD Birds dataset](http://www.vision.caltech.edu/visipedia/CUB-200.html)**

### Google Colab Setup

We will be using [Google Colab](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true), a free environment to run our experiments. If you have your own GPU and deep learning setup, you can also use your computers. If you choose Google Colab, here are instructions on how to get started:
1.	Open [Colab](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true), click on 'File' in the top left corner and select upload 'New Python 3 Notebook'. Upload a notebook (.ipynb) file in the code package.
2.	In your Google Drive, create a new folder, for example, “DeepNet”. This is the folder that will be mounted to Colab. All outputs generated by Colab Notebook will be saved here.
3.	Within the folder, create a subfolder called 'data'. Upload data files (cifar100.tar.gz, train.tar.gz, test.tar.gz), which [you can download here](https://drive.google.com/open?id=18n3s1BnaJgtLxYHPWnBF2WL0MoI-7JBr).
4.	Follow the instructions in the notebook to finish the setup.
Keep in mind that you need to keep your browser window open while running Colab. Colab does not allow long-running jobs but it should be sufficient for the requirements of this assignment (expected training time is about 5 minutes for Part 1 and 20 minutes for Part 2 with 50 epochs).

### Part 1

**Dataset**

For this part of the project, we will be working with the CIFAR100 dataset (already loaded above). This dataset consists of 60K 32x32 color images from 100 classes, with 600 images per class. There are 50K training images and 10K test images. The images in CIFAR100 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels.

We have modified the standard dataset to create our own CIFAR100 dataset which consists of 45K training images (450 of each class), 5K validation images (50 of each class), and 10K test images (100 of each class). The train and val datasets have labels while all the labels in the test set are set to 0.

**BaseNet**

BaseNet uses the following neural network layers:

* Convolutional, i.e. nn.Conv2d

* Pooling, e.g. nn.MaxPool2d

* Fully-connected (linear), i.e. nn.Linear

* Non-linear activations, e.g. nn.ReLU

* Normalization, e.g. nn.batchnorm2d

BaseNet consists of two convolutional modules (conv-relu-maxpool) and two linear layers. The precise architecture is defined below:

<p align="center">
Table 1: BaseNet architecture
</p>
<p align="center">
<img width="890" alt="BaseNet Layers" src="https://user-images.githubusercontent.com/52765136/190062450-a03b8108-4d25-4963-87e7-944bd6b38c55.png">
</p>

#### Improving BaseNet

Our goal is to edit the BaseNet class or make new classes for devising a more accurate deep net architecture. For improving the network, we try the following:

1. **Data normalization**: Normalizing input data makes training easier and more robust. Similar to normalized epipolar geometry estimation, data in this case too could be made zero mean and fixed standard deviation (sigma=1 is the to-go choice). We can use transforms.Normalize() with the right parameters to make the data well conditioned (zero mean, std dev=1) for improved training. After your edits, make sure that test_transform has the same data normalization parameters as train_transform. Also, we used a useful method for data augmentation proposed in [this paper](https://arxiv.org/abs/1805.09501): Create a search space of data augmentation policies, evaluating the quality of a particular policy directly on the dataset of interest.
2. **Data augmentation**: We tried using transforms.RandomCrop() and/or transforms.RandomHorizontalFlip() to augment training data. We shouldn't have any data augmentation in test_transform (val or test data is never augmented).
3. **Deeper network**: Following the guidelines laid out by [this lecture](http://cs231n.github.io/convolutional-networks/) on CNN, we experimented by adding more convolutional and fully connected layers. We added more conv layers with increasing output channels and also added more linear (fc) layers. We do not put a maxpool layer after every conv layer in our deeper network as it leads to too much loss of information. The precise architecture of the final network is defined below:


<p align="center">
Table 2: Improved DeepNet architecture
</p>
<p align="center">
<img width="472" alt="DeepNet 1" src="https://user-images.githubusercontent.com/52765136/190065051-6d325f26-51d3-44c2-9f75-8f4e52229409.png">
</p>
<p align="center">
<img width="474" alt="DeepNet 2" src="https://user-images.githubusercontent.com/52765136/190065058-81921714-f8db-46e4-a218-43ff54aa2202.png">
</p>

4. **Normalization layers**: Normalization layers help reduce overfitting and improve training of the model. [Pytorch's normalization layers](https://pytorch.org/docs/master/nn.html#normalization-functions) are an easy way of incorporating them in our model. We added normalization layers after conv layers (nn.BatchNorm2d). We added normalization layers after linear layers and experimented with inserting them before or after ReLU layers (nn.BatchNorm1d).
5. **Early stopping**: After how many epochs to stop training? [This answer on stackexhange](https://datascience.stackexchange.com/questions/18339/why-use-both-validation-set-and-test-set/18346#18346) is a good summary of using train-val-test splits to reduce overfitting. [This blog](https://elitedatascience.com/overfitting-in-machine-learning#how-to-prevent) is also a good reference for early stopping. We should never use the test-set in anything but the final evaluation. Seeing the train loss and validation accuracy plot, we decide for how many epochs to train our model. Not too many (as that leads to overfitting) and not too few (else our model hasn't learnt enough).

### Part 2

In this part, we will fine-tune a ResNet model pre-trained on ImageNet for classifying the Caltech-UCSD Birds dataset. This dataset consists of 200 categories of birds, with 3000 images in train and 3033 images in test. The instructions are provided in the notebook.

We experimented with the following:
1.	**Varying the hyperparameters**: based on how our model performs on train in the current setting.

2.	**ResNet as a fixed feature extractor**: We can use the ResNet pre-trained on ImageNet as a fixed feature extractor. We freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.

<p align="center">
Figure 1: ResNet as a fixed feature extractor - Train result
</p>
<p align="center">
<img width="407" alt="basic" src="https://user-images.githubusercontent.com/52765136/190067958-042e7319-9588-445f-9fd3-63a89bb2943d.png">
</p>

3.	**Fine-tuning the ResNet** To fine-tune the entire ResNet, instead of only training the final layer, we set the parameter RESNET_LAST_ONLY to False.

<p align="center">
Figure 2: Fine-tuned ResNet - Train result
</p>
<p align="center">
<img width="422" alt="tuned" src="https://user-images.githubusercontent.com/52765136/190068045-2d9e8b13-8dd8-411c-94c7-8afb7118bedf.png">
</p>
